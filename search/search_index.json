{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#childrens-speech-recognizer-spanish","title":"Children's Speech Recognizer Spanish","text":"<p>A cross platform (Android/iOS/MacOS) Spanish children's speech recognizer library, written in Flutter and leveraging the Kaldi framework. The speech recognizer library reads a buffer from a microphone device and converts spoken words into text in near-instant inference time with high accuracy. This library is also extensible to your own custom speech recognition model!</p> <p>Note</p> <p>Since our built-in default model was trained on children's speech, it may perform poorly on adult's speech.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Spanish speech-to-text through a Kaldi-based automatic speech recognition (ASR) model, trained on children's speech.</li> <li>Integrate speech-to-text model with mobile and desktop applications.</li> </ul>"},{"location":"#installation-setup","title":"Installation / Setup","text":"<ul> <li>Install Flutter SDK.</li> <li>Install Visual Studio Code.</li> <li>Open the project in Visual Studio Code, navigate to <code>lib/main.dart</code>.</li> <li>Launch an Android emulator or iOS simulator. Optionaly, you can also connect to a real device.</li> <li>Run the demo on Android/iOS/MacOS by going to the top navigation bar of VSCode, hit Run, then Start Debugging.</li> </ul>"},{"location":"#android","title":"Android","text":"<p>On Android, you will need to allow microphone permission in <code>AndroidManifest.xml</code> like so:</p> <pre><code>&lt;uses-feature android:name=\"android.hardware.microphone\" android:required=\"false\"/&gt;\n&lt;uses-permission android:name=\"android.permission.RECORD_AUDIO\"/&gt;\n</code></pre>"},{"location":"#ios","title":"iOS","text":"<p>Similarly on iOS/MacOS:</p> <ul> <li>Open Xcode</li> <li>Navigate to <code>Info.plist</code></li> <li>Add microphone permission <code>NSMicrophoneUsageDescription</code>. You can follow this guide.</li> </ul>"},{"location":"#how-to-use","title":"How to Use","text":""},{"location":"#flutter-sample-app","title":"Flutter Sample App","text":"<ul> <li>After setting up, run the app by pressing the <code>Load model</code> button and then <code>Start listening</code></li> <li>Speak into the microphone and the corresponding output text will be displayed in the text field.</li> <li>Press <code>Stop listening</code> to stop the app from listening.</li> </ul> main.dart<pre><code>import 'package:speech_recognizer/speech_recognizer.dart';\n\nclass _MyHomePageState implements SpeechListener { // (1)\n  final recognizer = SpeechController.shared;\n\n  void _load() async {\n    // ask for permission\n    final permissions = await SpeechController.shared.permissions(); // (2)\n    if (permissions == AudioSpeechPermission.undetermined) {\n      await SpeechController.shared.authorize();\n    }\n\n    if (await SpeechController.shared.permissions() !=\n        AudioSpeechPermission.authorized) {\n      return;\n    }\n\n    if (!_isInitialized) {\n      await SpeechController.shared.initSpeech('id'); // (3)\n      setState(() {\n        _isInitialized = true;\n      });\n\n      SpeechController.shared.addListener(this); // (4)\n    }\n  }\n\n  /// listen to speech events and print result in UI\n  void onResult(\n    String transcript, bool wasEndpoint, bool resetEndPos,\n    bool isVoiceActive, bool isNoSpeech) {\n    if (transcript.isEmpty) {\n      return;\n    }\n\n    print(transcript);\n    setState(() {\n      _decoded.insert(0, transcript);\n    });\n  }\n}\n</code></pre> <ol> <li>Setup listener by implements <code>SpeechListener</code> in your class.</li> <li>Ask for recording permission.</li> <li>Initialize Spanish recognizer model.</li> <li>Register listener in this class.</li> <li>Output text listener while speaking.</li> <li>Normalized result.</li> </ol>"},{"location":"#architecture","title":"Architecture","text":"<p>This library uses Flutter Platform Channels to enable communication between Dart (Flutter) and native code (Android/iOS). The architecture follows a three-layer design:</p>"},{"location":"#1-flutter-layer-dart","title":"1. Flutter Layer (Dart)","text":"<p>The Flutter layer provides a high-level API through the <code>SpeechController</code> class, which communicates with native platforms using:</p> <ul> <li>Method Channel (<code>com.bookbot/control</code>): For sending commands to native code</li> <li>Event Channel (<code>com.bookbot/event</code>): For receiving continuous speech recognition results</li> </ul> <pre><code>// Example: Flutter sends command to native platform\nawait methodChannel.invokeMethod('initSpeech', [language, profileId, wordMode]);\n\n// Example: Flutter receives events from native platform\neventChannel.receiveBroadcastStream().listen((event) {\n  final transcript = event['transcript'];\n  final wasEndpoint = event['wasEndpoint'];\n  // Process recognition results\n});\n</code></pre>"},{"location":"#2-platform-channel-bridge","title":"2. Platform Channel Bridge","text":"<p>Platform channels act as a bridge between Flutter and native code:</p> Channel Name Type Purpose <code>com.bookbot/control</code> MethodChannel Send commands (init, listen, stop, etc.) <code>com.bookbot/event</code> EventChannel Receive recognition results continuously <code>com.bookbot/levels</code> EventChannel Receive audio level updates <code>com.bookbot/recognizer</code> EventChannel Receive recognizer running status"},{"location":"#3-native-layer-androidios","title":"3. Native Layer (Android/iOS)","text":""},{"location":"#android-implementation-kotlin","title":"Android Implementation (Kotlin)","text":"<p>The Android native code in <code>SpeechController.kt</code> handles:</p> <ol> <li>Microphone Permission Management: Requests and checks <code>RECORD_AUDIO</code> permission</li> <li>Speech Recognition Service: Integrates with Sherpa-ONNX ASR engine</li> <li>Audio Processing: Captures audio from microphone using Android's audio APIs</li> <li>Real-time Recognition: Processes audio buffers and sends results back to Flutter</li> </ol> <pre><code>// Android: Registering the plugin\nclass MainActivity : FlutterActivity() {\n    override fun configureFlutterEngine(flutterEngine: FlutterEngine) {\n        speechController = SpeechController(this, lifecycle)\n        flutterEngine.plugins.add(speechController)\n    }\n}\n\n// Android: Handling method calls from Flutter\noverride fun onMethodCall(call: MethodCall, result: MethodChannel.Result) {\n    when (call.method) {\n        \"initSpeech\" -&gt; initSpeech(call.arguments as List&lt;String?&gt;, result)\n        \"listen\" -&gt; startSpeech()\n        \"stopListening\" -&gt; stopSpeech()\n        // ... other methods\n    }\n}\n\n// Android: Sending results back to Flutter\noverride fun onSpeechResult(result: String, wasEndpoint: Boolean, ...) {\n    eventSink?.success(hashMapOf(\n        \"transcript\" to result,\n        \"wasEndpoint\" to wasEndpoint,\n        \"isVoiceActive\" to isVoiceActive\n    ))\n}\n</code></pre>"},{"location":"#ios-implementation-swift","title":"iOS Implementation (Swift)","text":"<p>The iOS native code in <code>SpeechController.swift</code> handles:</p> <ol> <li>Audio Session Management: Configures <code>AVAudioSession</code> for recording</li> <li>Audio Engine: Uses <code>AVAudioEngine</code> to capture microphone input</li> <li>Voice Activity Detection (VAD): Detects speech vs silence using Sherpa-ONNX VAD</li> <li>Speech Recognition: Processes audio with Sherpa-ONNX ASR model</li> </ol> <pre><code>// iOS: Registering the plugin\npublic static func register(with registrar: FlutterPluginRegistrar) {\n    let channel = FlutterMethodChannel(\n        name: \"com.bookbot/control\",\n        binaryMessenger: messenger\n    )\n    registrar.addMethodCallDelegate(instance, channel: channel)\n\n    let eventChannel = FlutterEventChannel(\n        name: \"com.bookbot/event\",\n        binaryMessenger: messenger\n    )\n    eventChannel.setStreamHandler(instance)\n}\n\n// iOS: Handling method calls from Flutter\npublic func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {\n    switch call.method {\n    case \"initSpeech\":\n        initSpeech(profileId: profileId, language: language, ...)\n    case \"listen\":\n        startListening()\n    case \"stopListening\":\n        stopListening()\n    // ... other methods\n    }\n}\n\n// iOS: Processing audio buffers\nengine.inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: format) { buffer, _ in\n    self.recognize(buffer: buffer)\n}\n</code></pre>"},{"location":"#speech-recognition-flow","title":"Speech Recognition Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Flutter App (Dart)                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502          SpeechController.shared.listen()                 \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502 Method Channel\n                             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Native Platform (Android/iOS)                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  1. Request Microphone Permission                         \u2502  \u2502\n\u2502  \u2502  2. Initialize AVAudioEngine/AudioRecord                  \u2502  \u2502\n\u2502  \u2502  3. Load Sherpa-ONNX ASR Model                           \u2502  \u2502\n\u2502  \u2502  4. Start Capturing Audio (100ms buffers)                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                          \u2502                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Audio Buffer Processing:                                 \u2502  \u2502\n\u2502  \u2502  \u2022 Convert to 16kHz PCM Float32                          \u2502  \u2502\n\u2502  \u2502  \u2022 Run Voice Activity Detection (VAD)                    \u2502  \u2502\n\u2502  \u2502  \u2022 Feed to Sherpa-ONNX Recognizer                        \u2502  \u2502\n\u2502  \u2502  \u2022 Decode Speech \u2192 Text                                  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                          \u2502 Event Channel                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Flutter App (Dart)                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Receive Results: { transcript, wasEndpoint, ... }        \u2502  \u2502\n\u2502  \u2502  Update UI with recognized text                           \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#key-technical-details","title":"Key Technical Details","text":"<ol> <li>Audio Processing:</li> <li>Microphone captures raw audio at native sample rate (typically 48kHz)</li> <li>Audio is resampled to 16kHz for ASR model compatibility</li> <li> <p>Buffer duration: 100ms for optimal latency</p> </li> <li> <p>Voice Activity Detection (VAD):</p> </li> <li>Uses Silero VAD model with 25ms window size</li> <li>Detects speech/silence patterns: <code>[silence][speech][silence]</code></li> <li> <p>Patience counters prevent false endpoint detection</p> </li> <li> <p>Recognition Modes:</p> </li> <li>Phoneme Mode: Returns phonetic tokens for pronunciation analysis</li> <li> <p>Word Mode: Returns complete words for text transcription</p> </li> <li> <p>Thread Safety:</p> </li> <li>Android: Uses coroutines and synchronized blocks</li> <li>iOS: Uses dedicated DispatchQueues for recognition, audio, and level processing</li> </ol>"},{"location":"#file-structure","title":"File Structure","text":"Platform Code Function Flutter <code>speech_recognizer.dart</code> Interface API to communicate with native platform (Android/iOS/Mac). There are many speech recognizer methods, check <code>lib/main.dart</code> to know how to use them. All Platforms <code>asr/es</code> Speech model shared for all platforms. iOS/MacOS <code>SpeechController.swift</code> Native platform channel for speech recognizer on iOS/MacOS. It uses sherpa-onnx with custom model. Android <code>SpeechController.kt</code> Native platform channel for speech recognizer on android. It uses sherpa-onnx with custom model."},{"location":"#ui-automation-testing","title":"UI Automation Testing","text":"<ul> <li>Follow Installation / Setup guide</li> <li>Launch an Android emulator or iOS simulator</li> <li>Run <code>flutter test integration_test/app_test.dart</code> </li> </ul> <p>https://github.com/user-attachments/assets/46476c73-cfbb-442d-8e81-3199fe0f704d</p>"},{"location":"#helpful-links-resources","title":"Helpful Links &amp; Resources","text":"<ul> <li>Flutter developer document</li> <li>Android developer document</li> <li>iOS/MacOS developer document</li> </ul>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"#credits","title":"Credits","text":"<p>Sherpa-onnx Onnxruntime</p>"},{"location":"contributing/","title":"Contributing to Speech Recognizer Spanish","text":"<p>Hi there! Thanks for taking your time to contribute!</p> <p>We welcome everyone to contribute and we value each contribution, even the smallest ones! We want to make contributing to this project as easy and transparent as possible, whether it's:</p> <ul> <li>Reporting a bug</li> <li>Discussing the current state of the code</li> <li>Submitting a fix</li> <li>Proposing new features</li> <li>Becoming a maintainer</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be mindful to respect our Code of Conduct.</p>"},{"location":"contributing/#we-develop-with-github","title":"We Develop with Github","text":"<p>We use github to host code, to track issues and feature requests, as well as accept pull requests.</p>"},{"location":"contributing/#we-use-github-so-all-code-changes-happen-through-pull-requests","title":"We Use Github, So All Code Changes Happen Through Pull Requests","text":"<p>Pull requests are the best way to propose changes to the codebase. We actively welcome your pull requests:</p> <ol> <li>Fork the repo and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>If you've changed APIs, update the documentation.</li> <li>Ensure the test suite passes.</li> <li>Make sure your code lints.</li> <li>Issue that pull request!</li> </ol>"},{"location":"contributing/#any-contributions-you-make-will-be-under-the-apache-20-license","title":"Any contributions you make will be under the Apache 2.0 License","text":"<p>In short, when you submit code changes, your submissions are understood to be under the same Apache 2.0 License that covers the project. Feel free to contact the maintainers if that's a concern.</p>"},{"location":"contributing/#report-bugs-using-githubs-issues","title":"Report bugs using Github's issues","text":"<p>We use GitHub issues to track public bugs. Report a bug by opening a new issue.</p>"},{"location":"contributing/#write-bug-reports-with-detail-background-and-sample-code","title":"Write bug reports with detail, background, and sample code","text":"<p>This is an example of a good and thorough bug report.</p> <p>Great Bug Reports tend to have:</p> <ul> <li>A quick summary and/or background</li> <li>Steps to reproduce</li> <li>Be specific!</li> <li>Give sample code if you can.</li> <li>What you expected would happen</li> <li>What actually happens</li> <li>Notes (possibly including why you think this might be happening, or stuff you tried that didn't work)</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under its Apache 2.0 License.</p>"},{"location":"contributing/#references","title":"References","text":"<p>This document was adapted from the open-source contribution guidelines for Facebook's Draft</p>"},{"location":"speech_recognizer/doc/api/static-assets/readme/","title":"Dart documentation generator","text":"<p>This directory includes static sources used by the Dart documentation generator through the <code>dart doc</code> command.</p> <p>To learn more about generating and viewing the generated documentation, check out the [<code>dart doc</code> documentation][].</p>"},{"location":"speech_recognizer/doc/api/static-assets/readme/#third-party-resources","title":"Third-party resources","text":""},{"location":"speech_recognizer/doc/api/static-assets/readme/#highlightjs","title":"highlight.js","text":"<p>License: https://github.com/highlightjs/highlight.js/blob/main/LICENSE</p>"},{"location":"speech_recognizer/doc/api/static-assets/readme/#update","title":"Update","text":"<ol> <li>Visit https://highlightjs.org/download/</li> <li>Open the developer console.</li> <li>Copy the below code block and execute.</li> <li>Verify that the listed language are selected.</li> <li>Download and extract assets.</li> </ol> <pre><code>var selected = [\n  'bash',\n  'c',\n  'css',\n  'dart',\n  'diff',\n  'java',\n  'javascript',\n  'json',\n  'kotlin',\n  'markdown',\n  'objectivec',\n  'plaintext',\n  'shell',\n  'swift',\n  'xml', // also includes html\n  'yaml',\n];\ndocument.querySelectorAll('input[type=checkbox]').forEach(function (elem) {elem.checked = selected.includes(elem.value);});\n</code></pre>"}]}